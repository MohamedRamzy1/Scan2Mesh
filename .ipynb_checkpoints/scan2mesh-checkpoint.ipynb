{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0750b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from data.shapenet import ShapeNet\n",
    "from model.vertix_model import VertixModel\n",
    "import tqdm\n",
    "from util.visualization import visualize_pointcloud\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dda497",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vertices = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9321d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VertixModel(num_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d50bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertixModel(\n",
       "  (conv_layers_1): Sequential(\n",
       "    (0): Conv3d(5, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
       "    (7): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (10): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_layers_2): Sequential(\n",
       "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv3d(64, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (7): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=256, out_features=750, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116d2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ShapeNet(sdf_path=\"data/shapenet_dim32_sdf\",\n",
    "                         meshes_path=\"data/shapenet_reduced\",\n",
    "                         class_mapping= \"data/shape_info.json\",\n",
    "                         split=\"train\",\n",
    "                         threshold= num_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f9c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating statistics .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153528/153528 [01:30<00:00, 1693.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 153528\n",
      "Data for each threshold:\n",
      "For threshold 100 num of images 68280\n",
      "For threshold 150 num of images 93048\n",
      "For threshold 200 num of images 115260\n",
      "For threshold 250 num of images 123840\n",
      "For threshold 300 num of images 128508\n",
      "For threshold 350 num of images 131568\n",
      "For threshold 400 num of images 133608\n",
      "For threshold 450 num of images 135066\n",
      "For threshold 500 num of images 135894\n",
      "15918 images were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "statistics, not_found = train_dataset.calculate_statistics([100,150,200,250,300,350,400,450,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba29c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153528/153528 [00:00<00:00, 1147022.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 02933112 has 7800 shapes\n",
      "Class 03001627 has 30000 shapes\n",
      "Class 04256520 has 15600 shapes\n",
      "Class 04379243 has 30000 shapes\n",
      "Class 02691156 has 19800 shapes\n",
      "Class 04530566 has 9600 shapes\n",
      "Class 02958343 has 29628 shapes\n",
      "Class 03636649 has 11100 shapes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.calculate_class_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b35f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 153528\n",
      "Filtering data ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153528/153528 [01:31<00:00, 1670.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 123840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50e0a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123840/123840 [00:00<00:00, 1048700.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 03001627 has 25944 shapes\n",
      "Class 04256520 has 15198 shapes\n",
      "Class 04379243 has 28782 shapes\n",
      "Class 02691156 has 19590 shapes\n",
      "Class 04530566 has 8628 shapes\n",
      "Class 02958343 has 11106 shapes\n",
      "Class 03636649 has 7890 shapes\n",
      "Class 02933112 has 6702 shapes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.calculate_class_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a969311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Length of dataset: 153528\n",
      "Filtering data ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 48713/153528 [00:28<01:01, 1717.02it/s]"
     ]
    }
   ],
   "source": [
    "from training import vertix_train\n",
    "config = {\n",
    "    'experiment_name': 'vertix_training',\n",
    "    'device': 'cuda:0',  \n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 250,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "    'sdf_path': 'data/shapenet_dim32_sdf',\n",
    "    'meshes_path': 'data/shapenet_reduced',\n",
    "    'class_mapping': 'data/shape_info.json',\n",
    "    'split': 'train',\n",
    "    'num_vertices': num_vertices\n",
    "}\n",
    "vertix_train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e212e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
